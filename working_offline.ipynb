{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working offline\n",
    "\n",
    "The data security policies of some organisations mean that Splink sometimes must be used in an offline environment (i.e. no internet connection).\n",
    "\n",
    "This has two main implications:\n",
    "- Some of the charts rely on javascript libraries which are hosted online.  A workaround is needed for these to work offline (`splink.charts.save_offline_chart`)\n",
    "- The user cannot install `jars` from an online package repository like `maven` using the `'spark.jars.packages'` [config option](https://spark.apache.org/docs/latest/configuration.html#runtime-environment) and instead must download and reference local copies of these jars\n",
    "\n",
    "This notebook contains some code examples to demonstrate how to workaround these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charts\n",
    "\n",
    "`splink` provides a function called `splink.charts.save_offline_chart` which takes any charting output, and saves it to a standalone `.html` file which can be viewed in a web browser, or in an iFrame in JupyterLab.\n",
    "\n",
    "This `.html` file needs no internet connection.\n",
    "\n",
    "The following is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.charts import save_offline_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/11 05:51:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/01/11 05:51:22 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from utility_functions.demo_utils import get_spark\n",
    "spark = get_spark() # See utility_functions/demo_utils.py for how to set up Spark\n",
    "df = spark.read.csv(\"data/fake_1000.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink import Splink\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"dob\",\n",
    "            \"m_probabilities\": [0.38818904757499695, 0.6118109226226807],\n",
    "            \"u_probabilities\": [0.9997655749320984, 0.00023440067889168859],\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"city\",\n",
    "            \"m_probabilities\": [0.29216697812080383, 0.7078329920768738],\n",
    "            \"u_probabilities\": [0.9105007648468018, 0.08949924260377884],\n",
    "        }\n",
    "    ],\n",
    "    }\n",
    "\n",
    "linker = Splink(settings, df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chart saved to my_bayes_factor_chart.html\n",
      "\n",
      "To view in Jupyter you can use the following command:\n",
      "\n",
      "from IPython.display import IFrame\n",
      "IFrame(src=\"./my_bayes_factor_chart.html\", width=1000, height=500)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chart_output = linker.model.bayes_factor_chart()\n",
    "save_offline_chart(chart_output, filename=\"my_bayes_factor_chart.html\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now open `my_bayes_factor_chart.html` in a web browser, or view it in an iFrame as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"200\"\n",
       "            src=\"./my_bayes_factor_chart.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff6b23a5310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src=\"./my_bayes_factor_chart.html\", width=1000, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of version 2.0, Splink now bundles the scala similarity jar, that provides efficient scala-based implementations of common record linkage functions like `jaro-winkler`.\n",
    "\n",
    "Splink also provides a function which reports the location of this file.\n",
    "\n",
    "This `jar` must still be registered with Spark when the SparkContext is created, but Splink now provides more helpful error messages.\n",
    "\n",
    "An example follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/robinlinacre/anaconda3/lib/python3.8/site-packages/splink/jars/scala-udf-similarity-0.0.9.jar'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the location of the jar\n",
    "from splink.jar_location import similarity_jar_location\n",
    "similarity_jar_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/11 05:51:32 WARN SimpleFunctionRegistry: The function jaro_winkler_sim replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Set up Spark to use it\n",
    "path = similarity_jar_location()\n",
    "\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.context import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "# conf.set('spark.driver.extraClassPath', path) # Spark 2.x only, not needed in spark 3\n",
    "conf.set('spark.jars', path)\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "spark.udf.registerJavaFunction('jaro_winkler_sim','uk.gov.moj.dash.linkage.JaroWinklerSimilarity',types.DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comp_l</th>\n",
       "      <th>comp_r</th>\n",
       "      <th>jaro_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robin</td>\n",
       "      <td>Rob</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robin</td>\n",
       "      <td>Robin</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robin</td>\n",
       "      <td>Robyn</td>\n",
       "      <td>0.906667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comp_l comp_r  jaro_score\n",
       "0  Robin    Rob    0.906667\n",
       "1  Robin  Robin    1.000000\n",
       "2  Robin  Robyn    0.906667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "data_list = [\n",
    "    {\"comp_l\": 'Robin', \"comp_r\": 'Rob'},\n",
    "    {\"comp_l\": 'Robin', \"comp_r\": 'Robin'},\n",
    "    {\"comp_l\": 'Robin', \"comp_r\": 'Robyn'},\n",
    "        ]\n",
    "\n",
    "df = spark.createDataFrame(Row(**x) for x in data_list)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "sql = \"\"\"\n",
    "select comp_l, comp_r, jaro_winkler_sim(comp_l, comp_r) as jaro_score\n",
    "from df \n",
    "\"\"\"\n",
    "spark.sql(sql).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "For the clustering functionality offered by the `splink.cluster.clusters_at_thresholds` function, there are two requirements.  \n",
    "- The `graphframes` python library corresponding to the user's version of Spark\n",
    "- A graphframes `jar` corresponding to the user's version of Spark\n",
    "\n",
    "These libraries are separate from Splink and maintained by other programmers, and therefore are not bundled with Splink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphframes Python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python package `graphframes` is required to use the `splink.cluster.clusters_at_thresholds`\n",
    "\n",
    "For Spark `2.4.x`, you can `pip install graphframes=0.6.0` or download from https://github.com/graphframes/graphframes/tags\n",
    "\n",
    "For Spark `>=3.0.0`, the package version you need is not available from PyPi, and you should download the version corresponding to your version of Spark from https://github.com/graphframes/graphframes/releases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphframes jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suggested code for Python 2.4.5 is:\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "   .builder\n",
    "   .appName(\"my_app\")\n",
    "   .config('spark.driver.extraClassPath', 'jars/graphframes-0.6.0-spark2.3-s_2.11.jar,jars/scala-logging-api_2.11-2.1.2.jar,jars/scala-logging-slf4j_2.11-2.1.2.jar') # Spark 2.x only\n",
    "    .config('spark.jars', 'jars/graphframes-0.6.0-spark2.3-s_2.11.jar,jars/scala-logging-api_2.11-2.1.2.jar,jars/scala-logging-slf4j_2.11-2.1.2.jar')\n",
    "   .getOrCreate()\n",
    "   )\n",
    "\n",
    "spark.sparkContext.setCheckpointDir(\"graphframes_tempdir/\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Note extraClassPath is needed on spark version `2.x` only.  This line must be omitted in Spark `>=3.0.0` \n",
    "\n",
    "You can find these jars [here](https://github.com/moj-analytical-services/splink_graph/tree/master/jars)\n",
    "\n",
    "You can find a list of jars corresponding to different versions of Spark [here](https://mvnrepository.com/artifact/graphframes/graphframes?repo=spark-packages)\n",
    "\n",
    "More info on adding jars to Spark [here](https://spark.apache.org/docs/latest/configuration.html#runtime-environment).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

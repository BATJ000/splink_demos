{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising configuration and best practices\n",
    "\n",
    "It is rare that Splink's default settings will provide the best data linking results.  Significant improvements can usually be made with careful attention to data cleaning, comparison functions, and other configuration options.  \n",
    "\n",
    "This notebook contains advice about best practice to help users get the more accurate data linkage results. It is based on experiences in optimising real-world jobs.\n",
    "\n",
    "It focusses on introducing concepts and building intuition instead of providing extensive code or exhaustive details of every config option.  Please see the other examples in this repository for fully-working examples of code, and [here](https://moj-analytical-services.github.io/splink_settings_editor/) for a full list of configuration options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "The [EM algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) that Splink uses to estimate model parameters is a unsupervised machine learning algorithm.  Specifically, it learns an optimal set of matching weights from the dataset of record comparisons.  \n",
    "\n",
    "This gives us a framework for thinking about how best to use Splink because it means we can dip into the standard set tools and ways of thinking that we use for any machine learning problem.\n",
    "\n",
    "Two things are particularly relevant for optimising of a Splink job:\n",
    "\n",
    "- Feature engineering:  How to transform our data to help Splink learn as much as possible.\n",
    "- How can we avoid confusing Splink with bad data or configuration?  (Overfitting, converging to local rather than global maxima, etc.)\n",
    "\n",
    "A second ingredient to help structure our thoughts is an better understanding of what Splink is trying to learn.\n",
    "\n",
    "In a nutshell, Splink looks to exploit differences in match probability between subsets of record comparisons.  The greater the differences, the more accurate the matching.\n",
    "\n",
    "This means that the task of the user is generally to try and make these groups as different as possible (though without overfitting).\n",
    "\n",
    "These concepts go a long way to explaining most of the optimisations in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our example data\n",
    "\n",
    "In the remainder of this notebook we will build up an example settings dictionary, based on an example dataset like this which we want to dedupe:\n",
    "\n",
    "\n",
    "| first_name | surname | initials | gender | dob        | postcode | office           |\n",
    "|------------|---------|----------|--------|------------|----------|------------------|\n",
    "| robin      | linacre | rl       | M      | 2000-01-01 | TA12 9PD | Bristol          |\n",
    "| john       | smith   | js       | NULL   | 1955-02-03 | BA8      | Manchester       |\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Decide on a list of comparison columns\n",
    "\n",
    "The user must decide which columns to include in the list of `comparison_columns`.\n",
    "\n",
    "The comparisons columns are used to subset the data into different groups - for instance, the subset of record comparisons where first name matches is likely to contain a greater proportion of matches than the subset of record comparisons where first name does not match.\n",
    "\n",
    "It is usually best to include any columns containing information that may help us accept or reject a match.\n",
    "\n",
    "**⚠️ POTENTIAL TRAP ⚠️ :**  Do not include columns that repeat information in other columns because this will then be double counted. In this case, the person's initials should not be included as a separate comparison column.   \n",
    "\n",
    "Where columns are highly correlated, consider including only one, since the Fellegi Sunter model assumes independence.  Violation of this assumption can often result in some degree of double counting.  In this case, office is likely to be highly correlated with postcode, so we would advice against inclusion.\n",
    "\n",
    "\n",
    "At this stage, we have the following settings\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"surname\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"gender\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"postcode\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Chose the number of levels for each comparison column\n",
    "\n",
    "Next, the user much choose the number of levels for each elements of the `comparison_columns` list.  If omitted, it defaults to 2.\n",
    "\n",
    "Consider specifically the `first_name` and `gender` comparison columns:\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    ...\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\",\n",
    "            \"num_levels\": 3\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "            \"col_name\": \"gender\",\n",
    "            \"num_levels\": 2\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "What does `num_levels` mean and why may we want three for the name columns, but two for gender?\n",
    "\n",
    "We are seeking to exploit differenecs in the distribution of matches between different subsets.\n",
    "\n",
    "For a first_name field, it's reasonable to assume there may be different match rates among three groups:\n",
    "- The group of record comparisons where first name matches exactly.\n",
    "- The group of record comparisons where first names are similar but not exactly the same\n",
    "- The group of record comparisons where first names are not similar.\n",
    "\n",
    "`\"num_levels\": 3` creates these groups, enabling Splink to estimate different match weights for each group.\n",
    "\n",
    "Another way to understand why three levels is important is to consider the cost of using only two levels:\n",
    "- The group of record comparisons where first name matches exactly.\n",
    "- The group of record comparisons where first name does not match exactly.\n",
    "\n",
    "The later group contains both record comparisons where the name almost matches, and ones where names does match at all.  The algorithm has to 'take an average' - which will result in the estimated match probability for 'almost matches' being scored down too harshly, and the 'does not match at all' records not being scored down enough.\n",
    "\n",
    "For the gender column, which contains 'M', 'F', or null, only a two-level comparison is reasonable: it either matches or it doesn't.  \n",
    "\n",
    "### Tradeoffs\n",
    "\n",
    "Taken to its logical extreme, we could consider having a very large number of levels to account for subtle difference in distributions between groups of records with slighly different characterstics (e.g. one group for each possible value of edit distance).\n",
    "\n",
    "There are two downsides to increasing the number of levels:\n",
    "- Overfitting.  The more parameters you have, the more likely estimates are to be influenced by specific characteristics of your training data.  With many levels, some groups will be very small, which can lead to extreme parameter estimates.\n",
    "- Computational complexity.  More levels means longer compute times.\n",
    "\n",
    "Empirically, we have found that 3 or 4 levels is generally suitable for columns where string comparison functions are being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Customising the comparison \n",
    "\n",
    "The nbe\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "settings = {...\n",
    "            \"comparison_columns\": [\n",
    "                {            \n",
    "                \"custom_name\": \"postcode_office_custom\"\n",
    "                \"case_expression\": custom_sql_case_expression_goes_here,\n",
    "                \"custom_columns_used\": [\"postcode\", \"office\"],\n",
    "                \"num_levels\": 3\n",
    "                }\n",
    "                ]\n",
    "           }\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-8ef235231fce>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-8ef235231fce>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    A variety of string comparison functions are provided in a [jar](https://github.com/moj-analytical-services/splink/tree/master/jars) accompanying splink. A code snippet showing how to load them into Spark can be found [here](https://github.com/moj-analytical-services/splink_demos/blob/master/code_snippets/loading_jar.py).\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A variety of string comparison functions are provided in a [jar](https://github.com/moj-analytical-services/splink/tree/master/jars) accompanying splink. A code snippet showing how to load them into Spark can be found [here](https://github.com/moj-analytical-services/splink_demos/blob/master/code_snippets/loading_jar.py). \n",
    "\n",
    "The functions provided are:\n",
    "- **[Jaro-Winkler similarity](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance#Jaro%E2%80%93Winkler_Similarity).**  A normalised edit distance running from 0 (no match) to 1 (exact match). Places greater weight on earlier characters. Generally a good option for names and dictionary words.\n",
    "- [**Jaccard similarity**](https://en.wikipedia.org/wiki/Jaccard_index).  A token-based similarity measure that considers the number of common tokens.  May be appropriate for some unique identifiers such as a driving licence number that may be entered with error.\n",
    "- [**Cosine distance**](https://en.wikipedia.org/wiki/Cosine_similarity).  A measure of similarity between different substrings.  Particularly appropriate for longer text strings that cannot be parsed out into separate fields.\n",
    "\n",
    "Splink defaults to using [Jaro-Winkler similarity](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance#Jaro%E2%80%93Winkler_Similarity) for string comparisons, which is generally a good choice for names or dictionary words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use of multiple blocking rules\n",
    "\n",
    "Try and ensure a large number of comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "settings = {\n",
    "    ...\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\",\n",
    "            \"num_levels\": 3,\n",
    "            \"u_probabilities\": [\n",
    "                    0.7,\n",
    "                    0.2,\n",
    "                    0.1\n",
    "                ],\n",
    "            \"m_probabilities\": [\n",
    "                    0.1,\n",
    "                    0.2,\n",
    "                    0.7\n",
    "                ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of levels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Blocking rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding splink defaults\n",
    "\n",
    "Demo of complete settings dictionary function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Term frequency adjustments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    \n",
    "    custom_expression = \"\"\"\n",
    "    CASE\n",
    "    WHEN first_name_l is null or first_name_r is null THEN -1\n",
    "    WHEN first_name_ =  first_name_r THEN 2\n",
    "    WHEN dmetaphone(first_name_l) = dmetaphone(first_name_r) THEN 1\n",
    "    ELSE 0 \n",
    "    END\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    settings = {\n",
    "        \"comparison_columns\": [ \n",
    "            {\n",
    "            \"col_name\": \"first_name\"\n",
    "            \"num_levels\": 3,\n",
    "            \"case_expression\" custom_expression   \n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
